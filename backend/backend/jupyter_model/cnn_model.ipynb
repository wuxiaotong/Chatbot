{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from pandas import DataFrame\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Flatten, Input, Embedding, TimeDistributed, Conv1D, concatenate, Lambda, Dropout\n",
    "from keras import backend as K\n",
    "from keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_question = pd.read_csv('../data/FiQA/FiQA_train_question_doc_final.tsv', sep='\\t')\n",
    "train_question = pd.read_csv('../data/FiQA/FiQA_train_question_final.tsv', sep='\\t')\n",
    "train_doc = pd.read_csv('../data/FiQA/FiQA_train_doc_final.tsv', sep='\\t')\n",
    "vocabulary = pd.read_csv('../data/FiQA/vocabulary.csv')\n",
    "vocab_size = len(vocabulary)\n",
    "max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarity(similarity):\n",
    "    dot = lambda a, b: K.batch_dot(a, b, axes=1)\n",
    "    if similarity == 'cosine':\n",
    "            return lambda x: dot(x[0], x[1]) / K.maximum(K.sqrt(dot(x[0], x[0]) * dot(x[1], x[1])), K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_LSTM_model():\n",
    "    question = Input(shape=(max_length,), dtype='int32', name='question_base')\n",
    "    answer = Input(shape=(max_length,), dtype='int32', name='answer_base')\n",
    "\n",
    "    # add embedding layers\n",
    "    weights = np.load(self.config['initial_embed_weights'])\n",
    "    embedding = Embedding(input_dim=self.config['n_words'],\n",
    "                          output_dim=weights.shape[1],\n",
    "                          weights=[weights])\n",
    "    question_embedding = embedding(question)\n",
    "    answer_embedding = embedding(answer)\n",
    "\n",
    "    f_rnn = LSTM(141, return_sequences=True, implementation=1)\n",
    "    b_rnn = LSTM(141, return_sequences=True, implementation=1, go_backwards=True)\n",
    "\n",
    "    qf_rnn = f_rnn(question_embedding)\n",
    "    qb_rnn = b_rnn(question_embedding)\n",
    "    # question_pool = merge([qf_rnn, qb_rnn], mode='concat', concat_axis=-1)\n",
    "    question_pool = concatenate([qf_rnn, qb_rnn], axis=-1)\n",
    "\n",
    "    af_rnn = f_rnn(answer_embedding)\n",
    "    ab_rnn = b_rnn(answer_embedding)\n",
    "    # answer_pool = merge([af_rnn, ab_rnn], mode='concat', concat_axis=-1)\n",
    "    answer_pool = concatenate([af_rnn, ab_rnn], axis=-1)\n",
    "\n",
    "    # cnn\n",
    "    cnns = [Conv1D(kernel_size=kernel_size,\n",
    "                   filters=500,\n",
    "                   activation='tanh',\n",
    "                   padding='same') for kernel_size in [1, 2, 3, 5]]\n",
    "    # question_cnn = merge([cnn(question_pool) for cnn in cnns], mode='concat')\n",
    "    question_cnn = concatenate([cnn(question_pool) for cnn in cnns], axis=-1)\n",
    "    # answer_cnn = merge([cnn(answer_pool) for cnn in cnns], mode='concat')\n",
    "    answer_cnn = concatenate([cnn(answer_pool) for cnn in cnns], axis=-1)\n",
    "\n",
    "    maxpool = Lambda(lambda x: K.max(x, axis=1, keepdims=False), output_shape=lambda x: (x[0], x[2]))\n",
    "    maxpool.supports_masking = True\n",
    "    question_pool = maxpool(question_cnn)\n",
    "    answer_pool = maxpool(answer_cnn)\n",
    "    \n",
    "    #dropout layer\n",
    "    dropout = Dropout(0.2)\n",
    "    similarity = get_similarity('cosine')\n",
    "    qa_model = Lambda(similarity, output_shape=lambda _: (None, 1))([dropout(question_pool),\n",
    "                                                                     dropout(answer_pool)])\n",
    "    model = Model(inputs=[question, answer], outputs=qa_model, name='qa_model')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model():\n",
    "    question = Input(shape=(max_length,), dtype='int32', name='question_base')\n",
    "    answer = Input(shape=(max_length,), dtype='int32', name='answer_base')\n",
    "    \n",
    "    #embedding layer\n",
    "    embedding = Embedding(input_dim=vocab_size, output_dim=200)\n",
    "    question_embedding = embedding(question)\n",
    "    answer_embedding = embedding(answer)\n",
    "    \n",
    "    #hidden layer\n",
    "    hidden_layer = TimeDistributed(Dense(200, activation='tanh'))\n",
    "    question_hl = hidden_layer(question_embedding)\n",
    "    answer_hl = hidden_layer(answer_embedding)\n",
    "    \n",
    "    #cnn layer\n",
    "    cnns = [Conv1D(kernel_size=kernel_size,\n",
    "                       filters=100,\n",
    "                       activation='tanh',\n",
    "                       padding='same') for kernel_size in [2, 3, 5, 7]]\n",
    "    question_cnn = concatenate([cnn(question_hl) for cnn in cnns], axis=-1)\n",
    "    answer_cnn = concatenate([cnn(answer_hl) for cnn in cnns], axis=-1)\n",
    "    \n",
    "    #max pooling layer\n",
    "    maxpool = Lambda(lambda x: K.max(x, axis=1, keepdims=False), output_shape=lambda x: (x[0], x[2]))\n",
    "    question_pool = maxpool(question_cnn)\n",
    "    answer_pool = maxpool(answer_cnn)\n",
    "    \n",
    "    #dropout layer\n",
    "    dropout = Dropout(0.2)\n",
    "    similarity = get_similarity('cosine')\n",
    "    qa_model = Lambda(similarity, output_shape=lambda _: (None, 1))([dropout(question_pool),\n",
    "                                                                     dropout(answer_pool)])\n",
    "    model = Model(inputs=[question, answer], outputs=qa_model, name='qa_model')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data():\n",
    "#     qdic = train_question.set_index('qid').T.to_dict('list')\n",
    "#     docdic = train_doc.set_index('docid').T.to_dict('list')\n",
    "\n",
    "    #question id and the corresponding doc id\n",
    "    question_id_list = train_doc_question['qid']\n",
    "    doc_id_list = train_doc_question['docid']\n",
    "\n",
    "    questions = []\n",
    "    good_answers = []\n",
    "    bad_answers = []\n",
    "    train_doc_list = train_doc.dropna(axis=0, how='any')\n",
    "    train_doc_list = list(train_doc_list['doc'])\n",
    "    \n",
    "    for i in range(0, len(train_doc_question)):\n",
    "        doc_value = train_doc[train_doc.docid == doc_id_list[i]]['doc'].values[0]\n",
    "        if doc_value == doc_value:\n",
    "            question = train_question[train_question.qid == question_id_list[i]]['question'].values[0]\n",
    "            questions.append(question)\n",
    "            good_answers.append(doc_value)\n",
    "            bad_answers.append(random.choice(train_doc_list))\n",
    "\n",
    "    return [questions, good_answers, bad_answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, good_answers, bad_answers = get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_qs = [one_hot(d, vocab_size) for d in questions]\n",
    "padded_qs = pad_sequences(encoded_qs, maxlen=max_length, padding='post')\n",
    "\n",
    "encoded_good_answers = [one_hot(d, vocab_size) for d in good_answers]\n",
    "padded_good_answers = pad_sequences(encoded_good_answers, maxlen=max_length, padding='post')\n",
    "\n",
    "encoded_bad_answers = [one_hot(d, vocab_size) for d in bad_answers]\n",
    "padded_bad_answers = pad_sequences(encoded_bad_answers, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    }
   ],
   "source": [
    "#define the input of the model\n",
    "q_input = Input(shape=(max_length,), dtype='int32', name='question_base')\n",
    "good_answers_input = Input(shape=(max_length,), dtype='int32', name='good_answers_base')\n",
    "bad_answers_input = Input(shape=(max_length,), dtype='int32', name='bad_answers_base')\n",
    "\n",
    "# get the cnn model\n",
    "model = CNN_model()\n",
    "# model.summary()\n",
    "good_similarity = model([q_input, good_answers_input])\n",
    "bad_similarity = model([q_input, bad_answers_input])\n",
    "\n",
    "#define the loss function, simialrity with the good_answers \n",
    "#need to be larger while similarity with the bad_answers need to be smaller\n",
    "loss = Lambda(lambda x: K.relu(0.009 - x[0] + x[1]),\n",
    "                      output_shape=lambda x: x[0])([good_similarity, bad_similarity])\n",
    "training_model = Model(inputs=[q_input, good_answers_input, bad_answers_input], outputs=loss,\n",
    "                                name='training_model')\n",
    "\n",
    "training_model.compile(loss=lambda y_true, y_pred: y_pred, optimizer='adam')\n",
    "# training_model.summary()\n",
    "\n",
    "prediction_model = Model(inputs=[q_input, good_answers_input], outputs=good_similarity,\n",
    "                                      name='prediction_model')\n",
    "\n",
    "prediction_model = Model(inputs=[q_input, good_answers_input], outputs=good_similarity,\n",
    "                                      name='prediction_model')\n",
    "origin_weight = prediction_model.get_weights()\n",
    "prediction_model.compile(loss=lambda y_true, y_pred: y_pred, optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "17072/17072 [==============================] - 540s 32ms/step - loss: 0.0144\n",
      "Epoch 2/10\n",
      "17072/17072 [==============================] - 513s 30ms/step - loss: 0.0104\n",
      "Epoch 3/10\n",
      "17072/17072 [==============================] - 540s 32ms/step - loss: 0.0075\n",
      "Epoch 4/10\n",
      "17072/17072 [==============================] - 565s 33ms/step - loss: 0.0046\n",
      "Epoch 5/10\n",
      "17072/17072 [==============================] - 516s 30ms/step - loss: 0.0031\n",
      "Epoch 6/10\n",
      "17072/17072 [==============================] - 599s 35ms/step - loss: 0.0022\n",
      "Epoch 7/10\n",
      "17072/17072 [==============================] - 592s 35ms/step - loss: 0.0017\n",
      "Epoch 8/10\n",
      "17072/17072 [==============================] - 500s 29ms/step - loss: 0.0012\n",
      "Epoch 9/10\n",
      "17072/17072 [==============================] - 482s 28ms/step - loss: 0.0011\n",
      "Epoch 10/10\n",
      "17072/17072 [==============================] - 438s 26ms/step - loss: 9.4636e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13c97ad68>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y = np.zeros(shape=(len(encoded_qs),)) # doesn't get used\n",
    "\n",
    "training_model.fit([padded_qs, padded_good_answers, padded_bad_answers], y, batch_size=512, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_weight = prediction_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model.save('../model_result/cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model.load_weights('../model_result/cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49149403],\n",
       "       [0.51208264],\n",
       "       [0.5983377 ],\n",
       "       ...,\n",
       "       [0.6669734 ],\n",
       "       [0.58721685],\n",
       "       [0.5839964 ]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_model.predict([padded_qs, padded_good_answers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3205113 ],\n",
       "       [0.20022056],\n",
       "       [0.427616  ],\n",
       "       ...,\n",
       "       [0.22471167],\n",
       "       [0.4220238 ],\n",
       "       [0.16637047]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_model.predict([padded_qs, padded_bad_answers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = train_doc.dropna()\n",
    "select_qs = [train_question['question'][0]] * len(train_doc)\n",
    "select_encoded_qs = [one_hot(d, vocab_size) for d in select_qs]\n",
    "select_padded_qs = pad_sequences(select_encoded_qs, maxlen=max_length, padding='post')\n",
    "select_answers = train_doc['doc'].values \n",
    "select_encoded_answers = [one_hot(d, vocab_size) for d in select_answers]\n",
    "select_padded_answers = pad_sequences(select_encoded_answers, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "select_result = prediction_model.predict([select_padded_qs, select_padded_answers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 'What is considered a business expense on a business trip?'\n",
      " \"Nov 8 '11 at 15:14\"]\n",
      "0\n",
      "What is considered a business expense on a business trip?\n",
      "     docid   probability\n",
      "984  18850  [0.49149403]\n"
     ]
    }
   ],
   "source": [
    "result = DataFrame(data={'probability': list(select_result), 'docid': train_doc['docid'].values})\n",
    "result = result.sort_values('probability', ascending=False)\n",
    "result =  result.reset_index(drop=True)\n",
    "\n",
    "print(train_question.values[0])\n",
    "print(train_question['qid'][0])\n",
    "print(train_question['question'][0])\n",
    "print(result[result.docid == 18850])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  qid  docid\n",
      "0   0    0  18850\n",
      "57600\n"
     ]
    }
   ],
   "source": [
    "print(train_doc_question[train_doc_question.qid == train_question['qid'][0]])\n",
    "print(len(select_qs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_qs_02 = [train_question['question'][1]] * len(train_doc)\n",
    "select_encoded_qs_02 = [one_hot(d, vocab_size) for d in select_qs_02]\n",
    "select_padded_qs_02 = pad_sequences(select_encoded_qs_02, maxlen=max_length, padding='post')\n",
    "select_answers_02 = train_doc['doc'].values \n",
    "select_encoded_answers_02 = [one_hot(d, vocab_size) for d in select_answers_02]\n",
    "select_padded_answers_02 = pad_sequences(select_encoded_answers_02, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_result_02 = prediction_model.predict([select_padded_qs_02, select_padded_answers_02])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_02 = DataFrame(data={'probability': list(select_result_02), 'docid': train_doc['docid'].values})\n",
    "result_02 = result_02.sort_values('probability', ascending=False)\n",
    "result_02 =  result_02.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     docid  probability\n",
      "396  18850  [0.5503769]\n"
     ]
    }
   ],
   "source": [
    "print(result_02[result_02.docid == 18850])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_sample(padded_qs, padded_good_answers, padded_bad_answers, test_len, qs_len):\n",
    "    result = []\n",
    "    for i in range(0, test_len):\n",
    "        tmp_qs = np.array([padded_qs[i]]*qs_len)\n",
    "        tmp_answers = np.vstack((padded_good_answers[i],  random.sample(list(padded_bad_answers), qs_len - 1)))\n",
    "        result.append({'q': tmp_qs, 'answers': tmp_answers})\n",
    "    return result\n",
    "\n",
    "def test_accuracy(test_sample, prediction_model, recall_len):\n",
    "    accuracy = 0;\n",
    "    for item in test_sample:\n",
    "        result = prediction_model.predict([item['q'], item['answers']])\n",
    "        if np.argmax(result) < recall_len:\n",
    "            accuracy += 1\n",
    "    accuracy /= len(test_sample) \n",
    "    print(accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 1\n",
      "answer_length: 10\n",
      "0.76\n",
      "answer_length: 20\n",
      "0.69\n",
      "answer_length: 30\n",
      "0.61\n",
      "answer_length: 40\n",
      "0.6\n",
      "answer_length: 50\n",
      "0.48\n",
      "answer_length: 100\n",
      "0.39\n"
     ]
    }
   ],
   "source": [
    "recall = 1\n",
    "print('recall:', recall)\n",
    "print('answer_length: 10')\n",
    "test_sample_01 = create_test_sample(padded_qs, padded_good_answers, padded_bad_answers, 100, 10)\n",
    "accuracy = test_accuracy(test_sample_01, prediction_model, recall)\n",
    "print('answer_length: 20')\n",
    "test_sample_02 = create_test_sample(padded_qs, padded_good_answers, padded_bad_answers, 100, 20)\n",
    "accuracy = test_accuracy(test_sample_02, prediction_model, recall)\n",
    "print('answer_length: 30')\n",
    "test_sample_03 = create_test_sample(padded_qs, padded_good_answers, padded_bad_answers, 100, 30)\n",
    "accuracy = test_accuracy(test_sample_03, prediction_model, recall)\n",
    "print('answer_length: 40')\n",
    "test_sample_04 = create_test_sample(padded_qs, padded_good_answers, padded_bad_answers, 100, 40)\n",
    "accuracy = test_accuracy(test_sample_04, prediction_model, recall)\n",
    "print('answer_length: 50')\n",
    "test_sample_05 = create_test_sample(padded_qs, padded_good_answers, padded_bad_answers, 100, 50)\n",
    "accuracy = test_accuracy(test_sample_05, prediction_model, recall)\n",
    "print('answer_length: 100')\n",
    "test_sample_06 = create_test_sample(padded_qs, padded_good_answers, padded_bad_answers, 100, 100)\n",
    "accuracy = test_accuracy(test_sample_06, prediction_model, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 2\n",
      "answer_length: 10\n",
      "0.8\n",
      "answer_length: 20\n",
      "0.69\n",
      "answer_length: 30\n",
      "0.63\n",
      "answer_length: 40\n",
      "0.61\n",
      "answer_length: 50\n",
      "0.48\n",
      "answer_length: 100\n",
      "0.39\n"
     ]
    }
   ],
   "source": [
    "recall = 2\n",
    "print('recall:', recall)\n",
    "print('answer_length: 10')\n",
    "accuracy = test_accuracy(test_sample_01, prediction_model, recall)\n",
    "print('answer_length: 20')\n",
    "accuracy = test_accuracy(test_sample_02, prediction_model, recall)\n",
    "print('answer_length: 30')\n",
    "accuracy = test_accuracy(test_sample_03, prediction_model, recall)\n",
    "print('answer_length: 40')\n",
    "accuracy = test_accuracy(test_sample_04, prediction_model, recall)\n",
    "print('answer_length: 50')\n",
    "accuracy = test_accuracy(test_sample_05, prediction_model, recall)\n",
    "print('answer_length: 100')\n",
    "accuracy = test_accuracy(test_sample_06, prediction_model, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 3\n",
      "answer_length: 10\n",
      "0.82\n",
      "answer_length: 20\n",
      "0.71\n",
      "answer_length: 30\n",
      "0.65\n",
      "answer_length: 40\n",
      "0.65\n",
      "answer_length: 50\n",
      "0.49\n",
      "answer_length: 100\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "recall = 3\n",
    "print('recall:', recall)\n",
    "print('answer_length: 10')\n",
    "accuracy = test_accuracy(test_sample_01, prediction_model, recall)\n",
    "print('answer_length: 20')\n",
    "accuracy = test_accuracy(test_sample_02, prediction_model, recall)\n",
    "print('answer_length: 30')\n",
    "accuracy = test_accuracy(test_sample_03, prediction_model, recall)\n",
    "print('answer_length: 40')\n",
    "accuracy = test_accuracy(test_sample_04, prediction_model, recall)\n",
    "print('answer_length: 50')\n",
    "accuracy = test_accuracy(test_sample_05, prediction_model, recall)\n",
    "print('answer_length: 100')\n",
    "accuracy = test_accuracy(test_sample_06, prediction_model, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 4\n",
      "answer_length: 10\n",
      "0.85\n",
      "answer_length: 20\n",
      "0.71\n",
      "answer_length: 30\n",
      "0.66\n",
      "answer_length: 40\n",
      "0.65\n",
      "answer_length: 50\n",
      "0.5\n",
      "answer_length: 100\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "recall = 4\n",
    "print('recall:', recall)\n",
    "print('answer_length: 10')\n",
    "accuracy = test_accuracy(test_sample_01, prediction_model, recall)\n",
    "print('answer_length: 20')\n",
    "accuracy = test_accuracy(test_sample_02, prediction_model, recall)\n",
    "print('answer_length: 30')\n",
    "accuracy = test_accuracy(test_sample_03, prediction_model, recall)\n",
    "print('answer_length: 40')\n",
    "accuracy = test_accuracy(test_sample_04, prediction_model, recall)\n",
    "print('answer_length: 50')\n",
    "accuracy = test_accuracy(test_sample_05, prediction_model, recall)\n",
    "print('answer_length: 100')\n",
    "accuracy = test_accuracy(test_sample_06, prediction_model, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall: 5\n",
      "answer_length: 10\n",
      "0.87\n",
      "answer_length: 20\n",
      "0.72\n",
      "answer_length: 30\n",
      "0.67\n",
      "answer_length: 40\n",
      "0.67\n",
      "answer_length: 50\n",
      "0.5\n",
      "answer_length: 100\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "recall = 5\n",
    "print('recall:', recall)\n",
    "print('answer_length: 10')\n",
    "accuracy = test_accuracy(test_sample_01, prediction_model, recall)\n",
    "print('answer_length: 20')\n",
    "accuracy = test_accuracy(test_sample_02, prediction_model, recall)\n",
    "print('answer_length: 30')\n",
    "accuracy = test_accuracy(test_sample_03, prediction_model, recall)\n",
    "print('answer_length: 40')\n",
    "accuracy = test_accuracy(test_sample_04, prediction_model, recall)\n",
    "print('answer_length: 50')\n",
    "accuracy = test_accuracy(test_sample_05, prediction_model, recall)\n",
    "print('answer_length: 100')\n",
    "accuracy = test_accuracy(test_sample_06, prediction_model, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
