{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, LSTM\n",
    "from sklearn.utils import shuffle\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from stop_words import get_stop_words\n",
    "from pandas import Series, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train_baseline.csv')\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "data =  np.array(df['data'])\n",
    "y = np.array(df['label'])\n",
    "\n",
    "y = pd.get_dummies(y)\n",
    "x = []\n",
    "for i in data:\n",
    "    i = i.strip('[|]')\n",
    "    x.append(i.split(','))\n",
    "x = np.array(x)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5000\n",
      "1\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "train_x = x[0: 5000]\n",
    "train_y = y[0: 5000]\n",
    "print(sum(train_y))\n",
    "print(len(train_y))\n",
    "test_x = x[5000:]\n",
    "test_y = y[5000:]\n",
    "print(sum(test_y))\n",
    "print(len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=train_x.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5000/5000 [==============================] - 1s 181us/step - loss: 7.8824 - acc: 0.5076\n",
      "Epoch 2/50\n",
      "5000/5000 [==============================] - 1s 102us/step - loss: 8.2261 - acc: 0.4882\n",
      "Epoch 3/50\n",
      "5000/5000 [==============================] - 1s 102us/step - loss: 8.1767 - acc: 0.4920\n",
      "Epoch 4/50\n",
      "5000/5000 [==============================] - 1s 101us/step - loss: 8.0603 - acc: 0.4996\n",
      "Epoch 5/50\n",
      "5000/5000 [==============================] - 1s 114us/step - loss: 8.2358 - acc: 0.4878\n",
      "Epoch 6/50\n",
      "5000/5000 [==============================] - 1s 105us/step - loss: 8.2269 - acc: 0.4876\n",
      "Epoch 7/50\n",
      "5000/5000 [==============================] - 1s 103us/step - loss: 8.2645 - acc: 0.4844\n",
      "Epoch 8/50\n",
      "5000/5000 [==============================] - 1s 105us/step - loss: 8.2800 - acc: 0.4830\n",
      "Epoch 9/50\n",
      "5000/5000 [==============================] - 1s 120us/step - loss: 8.3454 - acc: 0.4804\n",
      "Epoch 10/50\n",
      "5000/5000 [==============================] - 1s 104us/step - loss: 8.2371 - acc: 0.4870\n",
      "Epoch 11/50\n",
      "5000/5000 [==============================] - 1s 114us/step - loss: 8.2355 - acc: 0.4862\n",
      "Epoch 12/50\n",
      "5000/5000 [==============================] - 1s 116us/step - loss: 8.1798 - acc: 0.4892\n",
      "Epoch 13/50\n",
      "5000/5000 [==============================] - 1s 109us/step - loss: 8.1860 - acc: 0.4874\n",
      "Epoch 14/50\n",
      "5000/5000 [==============================] - 1s 129us/step - loss: 7.9095 - acc: 0.5042\n",
      "Epoch 15/50\n",
      "5000/5000 [==============================] - 1s 143us/step - loss: 8.0504 - acc: 0.4954\n",
      "Epoch 16/50\n",
      "5000/5000 [==============================] - 1s 129us/step - loss: 8.1441 - acc: 0.4896\n",
      "Epoch 17/50\n",
      "5000/5000 [==============================] - 1s 129us/step - loss: 7.9986 - acc: 0.4986\n",
      "Epoch 18/50\n",
      "5000/5000 [==============================] - 1s 113us/step - loss: 8.0789 - acc: 0.4936\n",
      "Epoch 19/50\n",
      "5000/5000 [==============================] - 1s 128us/step - loss: 8.0241 - acc: 0.4968\n",
      "Epoch 20/50\n",
      "5000/5000 [==============================] - 1s 122us/step - loss: 7.9632 - acc: 0.5006\n",
      "Epoch 21/50\n",
      "5000/5000 [==============================] - 1s 105us/step - loss: 8.0138 - acc: 0.4972\n",
      "Epoch 22/50\n",
      "5000/5000 [==============================] - 1s 113us/step - loss: 8.0000 - acc: 0.4982\n",
      "Epoch 23/50\n",
      "5000/5000 [==============================] - 1s 113us/step - loss: 8.0000 - acc: 0.4982\n",
      "Epoch 24/50\n",
      "5000/5000 [==============================] - 1s 103us/step - loss: 7.9936 - acc: 0.4986\n",
      "Epoch 25/50\n",
      "5000/5000 [==============================] - 1s 133us/step - loss: 7.9968 - acc: 0.4984\n",
      "Epoch 26/50\n",
      "5000/5000 [==============================] - 1s 128us/step - loss: 7.9968 - acc: 0.4984\n",
      "Epoch 27/50\n",
      "5000/5000 [==============================] - 1s 110us/step - loss: 8.0096 - acc: 0.4976\n",
      "Epoch 28/50\n",
      "5000/5000 [==============================] - 1s 116us/step - loss: 7.9968 - acc: 0.4984\n",
      "Epoch 29/50\n",
      "5000/5000 [==============================] - 1s 109us/step - loss: 8.0032 - acc: 0.4980\n",
      "Epoch 30/50\n",
      "5000/5000 [==============================] - 1s 117us/step - loss: 8.0001 - acc: 0.4982\n",
      "Epoch 31/50\n",
      "5000/5000 [==============================] - 1s 122us/step - loss: 8.0000 - acc: 0.4982\n",
      "Epoch 32/50\n",
      "5000/5000 [==============================] - 1s 108us/step - loss: 7.9936 - acc: 0.4986\n",
      "Epoch 33/50\n",
      "5000/5000 [==============================] - 1s 113us/step - loss: 8.0064 - acc: 0.4978\n",
      "Epoch 34/50\n",
      "5000/5000 [==============================] - 1s 109us/step - loss: 7.9872 - acc: 0.4990\n",
      "Epoch 35/50\n",
      "5000/5000 [==============================] - 1s 104us/step - loss: 8.0064 - acc: 0.4978\n",
      "Epoch 36/50\n",
      "5000/5000 [==============================] - 1s 105us/step - loss: 8.0064 - acc: 0.4978\n",
      "Epoch 37/50\n",
      "5000/5000 [==============================] - 1s 128us/step - loss: 7.9970 - acc: 0.4984\n",
      "Epoch 38/50\n",
      "5000/5000 [==============================] - 1s 117us/step - loss: 7.9968 - acc: 0.4984\n",
      "Epoch 39/50\n",
      "5000/5000 [==============================] - 1s 155us/step - loss: 7.9968 - acc: 0.4984\n",
      "Epoch 40/50\n",
      "5000/5000 [==============================] - 1s 133us/step - loss: 7.9808 - acc: 0.4994\n",
      "Epoch 41/50\n",
      "5000/5000 [==============================] - 1s 107us/step - loss: 7.9968 - acc: 0.4984\n",
      "Epoch 42/50\n",
      "5000/5000 [==============================] - 1s 115us/step - loss: 8.0064 - acc: 0.4978\n",
      "Epoch 43/50\n",
      "5000/5000 [==============================] - 1s 103us/step - loss: 7.9904 - acc: 0.4988\n",
      "Epoch 44/50\n",
      "5000/5000 [==============================] - 1s 104us/step - loss: 7.9839 - acc: 0.4992\n",
      "Epoch 45/50\n",
      "5000/5000 [==============================] - 1s 106us/step - loss: 8.0000 - acc: 0.4982\n",
      "Epoch 46/50\n",
      "5000/5000 [==============================] - 1s 105us/step - loss: 8.0032 - acc: 0.4980\n",
      "Epoch 47/50\n",
      "5000/5000 [==============================] - 1s 106us/step - loss: 8.0000 - acc: 0.4982\n",
      "Epoch 48/50\n",
      "5000/5000 [==============================] - 1s 129us/step - loss: 8.0064 - acc: 0.4978\n",
      "Epoch 49/50\n",
      "5000/5000 [==============================] - 1s 124us/step - loss: 8.0032 - acc: 0.4980\n",
      "Epoch 50/50\n",
      "5000/5000 [==============================] - 1s 130us/step - loss: 7.9969 - acc: 0.4984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12503f128>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, epochs=50, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7072/7072 [==============================] - 1s 138us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.934271267636329, 0.6298076923076923]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#evaluate the model\n",
    "model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7072\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "predict_x = model.predict_proba(test_x)\n",
    "print(len(predict_x))\n",
    "print(predict_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = pd.read_csv('../data/vocabulary.csv')\n",
    "train_question = pd.read_csv('../data/FiQA_train_question_final.tsv', sep='\\t')\n",
    "train_doc = pd.read_csv('../data/FiQA_train_doc_final.tsv', sep='\\t')\n",
    "train_doc = train_doc.dropna()\n",
    "\n",
    "dictionary = dict(zip(list(vocabulary['token']), list(vocabulary['word'])))\n",
    "vocab_size = len(dictionary)\n",
    "VOCAB_PAD_ID = vocab_size + 1\n",
    "VOCAB_GO_ID = vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(lis, pad, size):\n",
    "    if size > len(lis):\n",
    "        lis += [pad] * (size - len(lis))\n",
    "    else:\n",
    "        lis = lis[0:size]\n",
    "    return lis\n",
    "    \n",
    "def text_to_id(texts, dictionary):\n",
    "    #get the dictionary of the vocalbulary in the corpus, the word_idct is the word and id mapping in the dictionary\n",
    "    # change the question and docs to id list\n",
    "    texts_id = []\n",
    "    for text in texts:\n",
    "        t = []\n",
    "        for word in text:\n",
    "            if word in dictionary:\n",
    "                t.append(dictionary[word])\n",
    "#             else:\n",
    "#                 print(word)\n",
    "        texts_id.append(t)\n",
    "    return texts_id\n",
    "\n",
    "def create_input(questions, docs, dictionary, isPositive = 1):\n",
    "    questions_id = text_to_id(questions, dictionary)\n",
    "    docs_id =  text_to_id(docs, dictionary)\n",
    "    # add the label to the corresponding question and docs\n",
    "    q_len = len(questions)\n",
    "    input_data = []\n",
    "    for i in range(0, q_len):\n",
    "        item = {}\n",
    "        item['question'] = questions_id[i]\n",
    "        item['doc'] = docs_id[i]\n",
    "        if isPositive:\n",
    "            item['label'] = [0, 1]\n",
    "        else:\n",
    "            item['label'] = [1, 0]\n",
    "        input_data.append(item)\n",
    "    return input_data\n",
    "\n",
    "def pack_question_n_utterance(q, doc, VOCAB_PAD_ID, VOCAB_GO_ID, q_length = 20, doc_length = 99):\n",
    "    q = padding(q, VOCAB_PAD_ID, q_length)\n",
    "    doc = padding(doc, VOCAB_PAD_ID, doc_length)\n",
    "    assert len(q) == q_length, \"question should be pad to q_length\"\n",
    "    assert len(doc) == doc_length, \"doc should be pad to doc_length\"\n",
    "    return q + [VOCAB_GO_ID] + doc\n",
    "\n",
    "def preprocess_data(data, VOCAB_PAD_ID, VOCAB_GO_ID, question_max_length = 20, doc_max_length = 99):\n",
    "    result_x = []\n",
    "    result_y = []\n",
    "    for o in data:\n",
    "        x = pack_question_n_utterance(o['question'], o['doc'], VOCAB_PAD_ID, VOCAB_GO_ID)\n",
    "        y_ = o['label']\n",
    "        assert len(x) == doc_max_length + question_max_length + 1, \"Wrong length afer padding\"\n",
    "        assert VOCAB_GO_ID in x, \"<GO> must be in input x\"\n",
    "        result_x.append(x)\n",
    "        result_y.append(y_)\n",
    "    \n",
    "    return [result_x, result_y]\n",
    "def splitWordByLibrary(documents):\n",
    "    texts = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    en_stop = get_stop_words('en')\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    for i in documents:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "        \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs = train_doc['doc'].values\n",
    "docs = splitWordByLibrary(docs) \n",
    "\n",
    "questions = train_question['question'].sample(1).values\n",
    "questions = splitWordByLibrary(questions)\n",
    "questions = questions*len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_data = create_input(questions, docs, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57600\n"
     ]
    }
   ],
   "source": [
    "print(len(input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = preprocess_data(input_data, VOCAB_PAD_ID, VOCAB_GO_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57600, 120)\n"
     ]
    }
   ],
   "source": [
    "x = np.array(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " ...\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(type(train_doc['docid'].values))\n",
    "print(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        docid           y\n",
      "0           3  [0.0, 1.0]\n",
      "1          31  [0.0, 1.0]\n",
      "2          56  [0.0, 1.0]\n",
      "3          59  [0.0, 1.0]\n",
      "4          63  [0.0, 1.0]\n",
      "5         100  [0.0, 1.0]\n",
      "6         108  [0.0, 1.0]\n",
      "7         125  [0.0, 1.0]\n",
      "8         132  [0.0, 1.0]\n",
      "9         138  [0.0, 1.0]\n",
      "10        158  [0.0, 1.0]\n",
      "11        164  [0.0, 1.0]\n",
      "12        178  [0.0, 1.0]\n",
      "13        180  [0.0, 1.0]\n",
      "14        222  [0.0, 1.0]\n",
      "15        225  [0.0, 1.0]\n",
      "16        239  [0.0, 1.0]\n",
      "17        264  [0.0, 1.0]\n",
      "18        277  [0.0, 1.0]\n",
      "19        288  [0.0, 1.0]\n",
      "20        289  [0.0, 1.0]\n",
      "21        294  [0.0, 1.0]\n",
      "22        297  [0.0, 1.0]\n",
      "23        302  [0.0, 1.0]\n",
      "24        316  [0.0, 1.0]\n",
      "25        330  [0.0, 1.0]\n",
      "26        339  [0.0, 1.0]\n",
      "27        343  [0.0, 1.0]\n",
      "28        357  [0.0, 1.0]\n",
      "29        396  [0.0, 1.0]\n",
      "...       ...         ...\n",
      "57570  599651  [0.0, 1.0]\n",
      "57571  599678  [0.0, 1.0]\n",
      "57572  599684  [0.0, 1.0]\n",
      "57573  599700  [0.0, 1.0]\n",
      "57574  599701  [0.0, 1.0]\n",
      "57575  599715  [0.0, 1.0]\n",
      "57576  599725  [0.0, 1.0]\n",
      "57577  599731  [0.0, 1.0]\n",
      "57578  599739  [0.0, 1.0]\n",
      "57579  599755  [0.0, 1.0]\n",
      "57580  599757  [0.0, 1.0]\n",
      "57581  599765  [0.0, 1.0]\n",
      "57582  599779  [0.0, 1.0]\n",
      "57583  599790  [0.0, 1.0]\n",
      "57584  599799  [0.0, 1.0]\n",
      "57585  599810  [0.0, 1.0]\n",
      "57586  599827  [0.0, 1.0]\n",
      "57587  599835  [0.0, 1.0]\n",
      "57588  599842  [0.0, 1.0]\n",
      "57589  599860  [0.0, 1.0]\n",
      "57590  599874  [0.0, 1.0]\n",
      "57591  599876  [0.0, 1.0]\n",
      "57592  599898  [0.0, 1.0]\n",
      "57593  599925  [0.0, 1.0]\n",
      "57594  599939  [0.0, 1.0]\n",
      "57595  599946  [0.0, 1.0]\n",
      "57596  599953  [0.0, 1.0]\n",
      "57597  599966  [0.0, 1.0]\n",
      "57598  599975  [0.0, 1.0]\n",
      "57599  599987  [0.0, 1.0]\n",
      "\n",
      "[57600 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df = DataFrame(data = {'docid': list(train_doc['docid'].values), 'y': list(y_)})\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
