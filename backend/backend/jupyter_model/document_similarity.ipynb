{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatterbot.logic import LogicAdapter\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import string\n",
    "import json\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.similarities import Similarity\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models import LdaModel\n",
    "from stop_words import get_stop_words\n",
    "from pandas import DataFrame\n",
    "from difflib import SequenceMatcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOME_FIXED_SEED = 43\n",
    "np.random.seed(SOME_FIXED_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x,y):\n",
    "    return math.sqrt(sum(pow(a-b,2) for a, b in zip(x, y)))\n",
    " \n",
    "def manhattan_distance(x,y):\n",
    "    return sum(abs(a-b) for a,b in zip(x,y))\n",
    " \n",
    "def jaccard_similarity(x,y):\n",
    "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "    union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "    return intersection_cardinality/float(union_cardinality)\n",
    "\n",
    "def cosine(vector1, vector2):\n",
    "    tmp = np.linalg.norm(vector1) * np.linalg.norm(vector2)\n",
    "    if tmp == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return float(np.dot(vector1,vector2) / tmp)\n",
    "    \n",
    "def KL(a, b):\n",
    "    a = np.asarray(a, dtype=np.float)\n",
    "    b = np.asarray(b, dtype=np.float)\n",
    "    return np.sum(np.where(a != 0, a * np.log(a / b), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_statement():\n",
    "    statements = pd.read_csv('../data/statements.csv', header=None, encoding = \"ISO-8859-1\")\n",
    "#     statements = pd.read_csv('../data/sample.csv', header=None, encoding = \"ISO-8859-1\")\n",
    "    return statements[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with open(\"../data/consumer_credit_data.json\",'r') as load_f:\n",
    "        load_dict = json.load(load_f, encoding='utf-8')\n",
    "        id_list = []\n",
    "        data_list = []\n",
    "\n",
    "        for item in load_dict['questions']:\n",
    "            id_list.append(item['_id'])\n",
    "            data_list.append(item['label'] + ' ' +item['externalComment']+ ' ' + item.get('tip', '')+ ' ' + item.get('internalComment', ''))\n",
    "\n",
    "        for item in load_dict['licences']:\n",
    "            if not(item.get('externalComment', '').find('This activity is exempted. You do not need to be authorised.')):\n",
    "                id_list.append(item['_id'])\n",
    "                data_list.append(item['label'] + ' ' + item.get('externalComment', ''))\n",
    "                \n",
    "    return [data_list, id_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitWordByLibrary(documents):\n",
    "    texts = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    en_stop = get_stop_words('en')\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    for i in documents:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "     \n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        \n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        \n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitWord(documents):\n",
    "    #split the sentence into word and remove the stop word\n",
    "    texts = []\n",
    "    stoplist=set('for a of the and to in at after with do i was am an Do its so need on if be were are is who we fca'.split())  \n",
    "    for document in documents:\n",
    "        document = document.translate(str.maketrans('','',string.punctuation))\n",
    "        tmp = []\n",
    "        for word in document.lower().split():\n",
    "            if word not in stoplist:\n",
    "                tmp.append(word)\n",
    "        texts.append(tmp)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_model(statements, data):\n",
    "#     statements = load_statement()\n",
    "    lda_result = []\n",
    "    lda_id = []\n",
    "    id_list = data[1]\n",
    "    tag_list= data[0]\n",
    "    \n",
    "    texts = splitWordByLibrary(tag_list)\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    ldamodel = LdaModel(corpus, num_topics=len(corpus), id2word = dictionary) \n",
    "    corpus_len = len(corpus)\n",
    "    statements = splitWordByLibrary(statements) \n",
    "    for statement in statements:\n",
    "        cos_sim_list = []\n",
    "#         statement = statement.translate(str.maketrans('','',string.punctuation))\n",
    "        for i in range(0, corpus_len):\n",
    "            new_vec = dictionary.doc2bow(statement)\n",
    "            dict1 = dict(ldamodel[new_vec])\n",
    "            dict2 = dict(ldamodel[corpus[i]])\n",
    "            vec2 = np.zeros(corpus_len)\n",
    "            vec1 = np.zeros(corpus_len)\n",
    "            for a in dict2:\n",
    "                vec2[a] = dict2[a]\n",
    "            for a in dict1:\n",
    "                vec1[a] = dict1[a]\n",
    "            cos_sim_list.append(cosine(vec1, vec2))\n",
    "        largest_index = np.argmax(cos_sim_list)\n",
    "        lda_result.append(tag_list[largest_index])\n",
    "        lda_id.append(id_list[largest_index])\n",
    "    return [lda_result, lda_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difflib_model(statements, data):\n",
    "    difflib_result = []\n",
    "    difflib_id = []\n",
    "    id_list = data[1]\n",
    "    tag_list= data[0]\n",
    "    tags = splitWordByLibrary(tag_list)\n",
    "    statements = splitWordByLibrary(statements)\n",
    "    for statement in statements:\n",
    "        largest_similarity = 0\n",
    "        largest_index = 0\n",
    "        for i in range(0, len(tags)):\n",
    "            similarity = SequenceMatcher(\n",
    "                None,\n",
    "                \"\".join(statement),\n",
    "                \"\".join(tags[i])\n",
    "            )\n",
    "            if similarity.ratio() > largest_similarity:\n",
    "                largest_similarity = similarity.ratio()\n",
    "                largest_index = i\n",
    "                \n",
    "        difflib_id.append(id_list[largest_index])\n",
    "        difflib_result.append(tag_list[largest_index])\n",
    "    return [difflib_result, difflib_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_model(statements, data):\n",
    "    #create dummy data \n",
    "    \n",
    "    tfidf_result = []\n",
    "    tfidf_id = []\n",
    "    id_list = data[1]\n",
    "    tag_list= data[0]\n",
    "    tags = splitWordByLibrary(tag_list)\n",
    "    dictionary = corpora.Dictionary(tags)\n",
    "    corpus = [dictionary.doc2bow(tag) for tag in tags]\n",
    "    corpus_len = len(dictionary)\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    index = similarities.MatrixSimilarity(tfidf[corpus], num_features=len(dictionary))\n",
    "    statements = splitWordByLibrary(statements)\n",
    "    for statement in statements:\n",
    "        cos_sim_list = []\n",
    "        euclidean_distance_list = []\n",
    "        manhattan_distance_list = []\n",
    "        jaccard_similarity_list = []\n",
    "        word_count_list = []\n",
    "        for i in range(0, len(tags)):\n",
    "            new_vec = dictionary.doc2bow(statement)\n",
    "            dict1 = dict(tfidf[new_vec])\n",
    "            dict2 = dict(tfidf[corpus[i]])\n",
    "\n",
    "            vec2 = np.zeros(corpus_len)\n",
    "            vec1 = np.zeros(corpus_len)\n",
    "\n",
    "            for a in dict2:\n",
    "                vec2[a] = dict2[a]\n",
    "            for a in dict1:\n",
    "                vec1[a] = dict1[a]\n",
    "                \n",
    "            cos_sim_list.append(cosine(vec1, vec2))\n",
    "            euclidean_distance_list.append(euclidean_distance(vec1, vec2))\n",
    "            jaccard_similarity_list.append(jaccard_similarity(vec1, vec2))\n",
    "            manhattan_distance_list.append(manhattan_distance(vec1, vec2))\n",
    "            # word_count_list.append(len(dict1)+len(dict2))\n",
    "        largest_index = np.argmax(cos_sim_list)\n",
    "        tfidf_result.append(tag_list[largest_index])\n",
    "        tfidf_id.append(id_list[largest_index])\n",
    "    return [tfidf_result, tfidf_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsi_model(statements, data):\n",
    "#     statements = load_statement()\n",
    "    lsi_result = []\n",
    "    lsi_id = []\n",
    "    id_list = data[1]\n",
    "    tag_list = data[0]\n",
    "\n",
    "    tags = splitWordByLibrary(tag_list)\n",
    "    dictionary = corpora.Dictionary(tags)\n",
    "    corpus = [dictionary.doc2bow(tag) for tag in tags]\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=len(tag_list))\n",
    "    index = similarities.MatrixSimilarity(lsi[corpus_tfidf]) \n",
    "    statements = splitWordByLibrary(statements)\n",
    "\n",
    "    for statement in statements:\n",
    "#         statement = statement.translate(str.maketrans('','',string.punctuation))\n",
    "        test_statement = dictionary.doc2bow(statement)\n",
    "        vec_lsi = lsi[test_statement]\n",
    "        sims = index[tfidf[vec_lsi]]\n",
    "        sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "        largest_index = sims[0][0]\n",
    "        lsi_result.append(tag_list[largest_index])\n",
    "        lsi_id.append(id_list[largest_index])\n",
    "    return [lsi_result, lsi_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_model(statements, data):\n",
    "    doc_result = []\n",
    "    doc_id = []\n",
    "    id_list = data[1]\n",
    "    tag_list = data[0]\n",
    "    texts = splitWordByLibrary(tag_list)\n",
    "    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(texts)]\n",
    "    model = Doc2Vec(documents)\n",
    "#     model = Doc2Vec(documents, vector_size=5, window=120, workers=4)\n",
    "    statements = splitWordByLibrary(statements) \n",
    "    for statement in statements:\n",
    "        infer_vector = model.infer_vector(statement)\n",
    "        index = model.docvecs.most_similar([infer_vector], topn = 1)[0][0]\n",
    "        doc_result.append(tag_list[index])\n",
    "        doc_id.append(id_list[index])\n",
    "        \n",
    "    return [doc_result, doc_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data()\n",
    "statements = load_statement()\n",
    "tfidf_result, tfidf_id = tfidf_model(statements, data)\n",
    "doc_result, doc_id = doc_model(statements, data)\n",
    "lda_result, lda_id = lda_model(statements, data)\n",
    "lsi_result, lsi_id = lsi_model(statements, data)\n",
    "difflib_result, difflib_id = difflib_model(statements, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'statements': np.array(statements), \n",
    "     'tiidf_result': np.array(tfidf_result), \n",
    "     'lsi_result': np.array(lsi_result),\n",
    "     'lda_result': np.array(lda_result), \n",
    "#      'doc_result': np.array(doc_result),\n",
    "     'difflib_resutl': np.array(doc_result), }\n",
    "df=DataFrame(data = d, columns = ['statements', 'tiidf_result', 'lsi_result', 'lda_result', 'difflib_resutl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/result/similarity.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create data for testing\n",
    "create_data = [['Hello','Hi',\n",
    "                'Greetings!','How is it going?',\n",
    "                'How are you doing?','Nice to meet you.',\n",
    "                'How do you do?','Hi, nice to meet you.',\n",
    "                'It is a pleasure to meet you.','Top of the morning to you!',\n",
    "                'Top of the morning to you!','what is good to eat?',\n",
    "                'do you drink','are you experiencing an energy shortage?',\n",
    "                'why can you not eat?','do you like being a chatterbot',\n",
    "                'if you could eat food, what would you eat?','do you wish you could eat food?',\n",
    "                'can a robot get drunk?','i like wine, do you?',\n",
    "                'what do robots need to survive?','will robots ever be able to eat?',\n",
    "                'do you want to go to FinTech page', 'do you want to go to RegTech page',\n",
    "                'What are your interests','What are your favorite subjects',\n",
    "                'What is your number','What is your favorite number',\n",
    "                'What is your location', 'Where do you live',\n",
    "                'Where are you from', 'Where are you',\n",
    "                'Do you have any brothers','Do you have any brothers',\n",
    "                'Who is your father','Who is your mother',\n",
    "                'Who is your boss','What is your age'\n",
    "               ],\n",
    "               ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11',\n",
    "                '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22',\n",
    "                '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33',\n",
    "                '34', '35', '36', '37', '38']\n",
    "              ]\n",
    "\n",
    "\n",
    "\n",
    "create_satatements = ['Hello bot','Hi bot',\n",
    "                      'Greeting','How is it going?',\n",
    "                      'How are you', 'Nice to meet you!',\n",
    "                      'How do you do?','Hi, nice to meet you too',\n",
    "                      'It is a pleasure to meet you','morning!!!',\n",
    "                      'top to you', 'what to eat',\n",
    "                      'do you like drinking', 'energy shortage',\n",
    "                      'why not eat', 'do you like chatterbot',\n",
    "                      'what do you eat apart from food', 'do you wish to take food',\n",
    "                      'can you get drunk', 'I like wine',\n",
    "                      'what do you need to survice', 'do you able eat?',\n",
    "                      'FinTech', 'RegTech',\n",
    "                      'what is your interest', 'tell me your favorite subjects',\n",
    "                      'tell me your number', 'tell me your favorite number',\n",
    "                      'tell me your location', 'do you live on earth',\n",
    "                      'where are you from', 'where are you',\n",
    "                      'do you have brothers', 'do you have sisters or brothers',\n",
    "                      'who is your dad', 'tell me your mother',\n",
    "                      'who is your boss', 'how old are you']\n",
    "               \n",
    "\n",
    "\n",
    "create_correct_id = create_data[1]\n",
    "tfidf_result, tfidf_id = tfidf_model(create_satatements, create_data)\n",
    "lda_result, lda_id = lda_model(create_satatements, create_data)\n",
    "lsi_result, lsi_id = lsi_model(create_satatements, create_data)\n",
    "doc_result, doc_id = doc_model(create_satatements, create_data)\n",
    "difflib_result, difflib_id = difflib_model(create_satatements, create_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_id = np.array(tfidf_id).reshape(-1, 1)\n",
    "create_correct_id = np.array(create_correct_id).reshape(-1, 1)\n",
    "lda_id = np.array(lda_id).reshape(-1, 1)\n",
    "lsi_id = np.array(lsi_id).reshape(-1, 1) \n",
    "doc_id = np.array(doc_id).reshape(-1, 1) \n",
    "difflib_id = np.array(difflib_id).reshape(-1, 1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73684211]\n",
      "[0.57894737]\n",
      "[0.71052632]\n",
      "[0.]\n",
      "[0.76315789]\n"
     ]
    }
   ],
   "source": [
    "print(sum(tfidf_id== create_correct_id)/len(create_correct_id))\n",
    "print(sum(lda_id== create_correct_id)/len(create_correct_id))\n",
    "print(sum(lsi_id== create_correct_id)/len(create_correct_id))\n",
    "print(sum(doc_id== create_correct_id)/len(create_correct_id))\n",
    "print(sum(difflib_id== create_correct_id)/len(create_correct_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
