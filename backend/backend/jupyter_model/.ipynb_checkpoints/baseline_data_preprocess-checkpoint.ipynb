{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from gensim import corpora, models, similarities\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from stop_words import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_question = pd.read_csv('../data/FiQA_train_question_doc_final.tsv', sep='\\t')\n",
    "train_question = pd.read_csv('../data/FiQA_train_question_final.tsv', sep='\\t')\n",
    "train_doc = pd.read_csv('../data/FiQA_train_doc_final.tsv', sep='\\t')\n",
    "vocabulary = pd.read_csv('../data/vocabulary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dict(zip(vocabulary['token'], vocabulary['word']))\n",
    "vocab_size = len(dictionary)\n",
    "VOCAB_PAD_ID = vocab_size + 1\n",
    "VOCAB_GO_ID = vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitWordByLibrary(documents):\n",
    "    texts = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    en_stop = get_stop_words('en')\n",
    "    p_stemmer = PorterStemmer()\n",
    "\n",
    "    for i in documents:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "        \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(lis, pad, size):\n",
    "    if size > len(lis):\n",
    "        lis += [pad] * (size - len(lis))\n",
    "    else:\n",
    "        lis = lis[0:size]\n",
    "    return lis\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_question_n_utterance(q, doc, VOCAB_PAD_ID, VOCAB_GO_ID, q_length = 20, doc_length = 99):\n",
    "    q = padding(q, VOCAB_PAD_ID, q_length)\n",
    "    doc = padding(doc, VOCAB_PAD_ID, doc_length)\n",
    "    assert len(q) == q_length, \"question should be pad to q_length\"\n",
    "    assert len(doc) == doc_length, \"doc should be pad to doc_length\"\n",
    "    return q + [VOCAB_GO_ID] + doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data, VOCAB_PAD_ID, VOCAB_GO_ID, question_max_length = 20, doc_max_length = 99):\n",
    "    result = []\n",
    "    for o in data:\n",
    "        x = pack_question_n_utterance(o['question'], o['doc'], VOCAB_PAD_ID, VOCAB_GO_ID)\n",
    "        y_ = o['label']\n",
    "        assert len(x) == doc_max_length + question_max_length + 1, \"Wrong length afer padding\"\n",
    "        assert VOCAB_GO_ID in x, \"<GO> must be in input x\"\n",
    "        result.append([x, y_])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_id(texts, dictionary):\n",
    "    #get the dictionary of the vocalbulary in the corpus, the word_idct is the word and id mapping in the dictionary\n",
    "#     word_dict = dictionary.token2id\n",
    "    # change the question and docs to id list\n",
    "    texts_id = []\n",
    "    for text in texts:\n",
    "        t = []\n",
    "        for word in text:\n",
    "            t.append(dictionary[word])\n",
    "        texts_id.append(t)\n",
    "    return texts_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input(questions, docs, dictionary, isPositive = 1):\n",
    "    questions_id = text_to_id(questions, dictionary)\n",
    "    docs_id =  text_to_id(docs, dictionary)\n",
    "    # add the label to the corresponding question and docs\n",
    "    q_len = len(questions)\n",
    "    input_data = []\n",
    "    for i in range(0, q_len):\n",
    "        item = {}\n",
    "        item['question'] = questions[i]\n",
    "        item['doc'] = docs[i]\n",
    "        item['label'] = isPositive\n",
    "        input_data.append(item)\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_negative_input(questions, docs, nums):\n",
    "    negative_questions = random.sample(list, nums)\n",
    "    negative_docs =  random.sample(list, nums)\n",
    "    \n",
    "    return [negative_questions, negative_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data()\n",
    "    qdic = train_question.set_index('qid').T.to_dict('list')\n",
    "    docdic = train_doc.set_index('docid').T.to_dict('list')\n",
    "\n",
    "    #question id and the corresponding doc id\n",
    "    question_id_list = train_doc_question['qid']\n",
    "    doc_id_list = train_doc_question['docid']\n",
    "\n",
    "    questions = []\n",
    "    docs = []\n",
    "\n",
    "    for i in range(0, len(question_id_list)):\n",
    "    #     question = train_question[train_question.qid == question_id_list[i]]['question'].values[0]\n",
    "    #     doc = train_doc[train_doc.docid == doc_id_list[i]]['doc'].values[0]\n",
    "        question = qdic[question_id_list[i]][1]\n",
    "        doc = docdic[doc_id_list[i]][1]\n",
    "        questions.append(question)\n",
    "        docs.append(doc)\n",
    "    return [question, docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, docs =  get_train_data()\n",
    "positive_questions = splitWordByLibrary(np.array(questions))\n",
    "positive_docs  = splitWordByLibrary(np.array(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_questions, negative_docs = create_negative_input(positive_questions, positive_docs, 1000):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'positive_questions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-149f9d4a9291>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpositive_input_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_questions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnegative_input_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_questions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_input_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative_input_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB_PAD_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVOCAB_GO_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'positive_questions' is not defined"
     ]
    }
   ],
   "source": [
    "positive_input_data = create_input(positive_questions, positive_docs, dictionary)\n",
    "negative_input_data = create_input(negative_questions, negative_docs, dictionary) \n",
    "input_data = np.vstack((positive_input_data, negative_input_data))\n",
    "result = preprocess_data(input_data, VOCAB_PAD_ID, VOCAB_GO_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=DataFrame(data=np.array(result))\n",
    "df.columns = ['data','label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/train_baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
